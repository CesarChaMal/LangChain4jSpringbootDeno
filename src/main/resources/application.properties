# Ollama API configuration
spring.ai.ollama.chat.enabled=true
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.options.model=llama2
spring.ai.ollama.chat.options.temperature=0.7
#spring.ai.ollama.chat.options.keep_alive=5m

# Logback configuration
#logging.level.org.springframework=DEBUG
logging.level.org.springframework=INFO
